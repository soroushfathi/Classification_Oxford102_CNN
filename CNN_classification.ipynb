{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rpWI3cRGPTX"
      },
      "source": [
        "# phase 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0ddSnxuGLAi"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRxbohbATDdy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "# import utils\n",
        "from typing import List\n",
        "from torchvision import transforms, models, datasets\n",
        "import torch\n",
        "from torchinfo import summary\n",
        "from torch.utils.data import DataLoader, Subset, random_split, ConcatDataset\n",
        "import numpy as np\n",
        "import random\n",
        "from os.path import exists\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UkOP_xTGApf"
      },
      "source": [
        "## GPU state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcJYNoSIqHWw",
        "outputId": "60a17609-e35c-4a46-93b8-b821448643dd"
      },
      "outputs": [],
      "source": [
        "use_gpu = True\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GIL20ZmUfhD",
        "outputId": "1ac6dc8c-4a88-4925-a2aa-02373672ca38"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "print(\"CUDA available: \", torch.cuda.is_available())\n",
        "\n",
        "# Check if code is running on GPU\n",
        "print(\"Code running on GPU: \", torch.cuda.is_initialized())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B03Ai5UkTDd0"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F07yw0wNTDd2"
      },
      "outputs": [],
      "source": [
        "def get_oxford_splits(\n",
        "    batch_size: int,\n",
        "    data_loader_seed: int = 111,\n",
        "    pin_memory: bool = True,\n",
        "    num_workers: int = 2,\n",
        "    ):\n",
        "    K = 5\n",
        "    num_support = 80\n",
        "    num_query = 20\n",
        "\n",
        "    def seed_worker(worker_id):\n",
        "        # worker_seed = torch.initial_seed() % 2 ** 32\n",
        "        np.random.seed(data_loader_seed)\n",
        "        random.seed(data_loader_seed)\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(data_loader_seed)\n",
        "\n",
        "    support_classes = list(np.arange(num_support))\n",
        "    query_classes = list(np.arange(num_query) + num_support)\n",
        "\n",
        "    img_dim = 64\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_dim, img_dim)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
        "    test_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_dim, img_dim)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
        "    validation_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_dim, img_dim)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
        "\n",
        "    data_path = f'/content/data'\n",
        "\n",
        "    train_ds_full = datasets.Flowers102(root=data_path, split=\"train\", download=True, transform=train_transforms)\n",
        "    val_ds_full = datasets.Flowers102(root=data_path, split=\"val\", download=True, transform=validation_transforms)\n",
        "    test_ds_full = datasets.Flowers102(root=data_path, split=\"test\", download=True, transform=test_transforms)\n",
        "\n",
        "    train_indxs_support = torch.where(torch.isin(torch.tensor(train_ds_full._labels), torch.asarray(support_classes)))[0]\n",
        "    val_indxs_support = torch.where(torch.isin(torch.tensor(val_ds_full._labels), torch.asarray(support_classes)))[0]\n",
        "    test_indxs_support = torch.where(torch.isin(torch.tensor(test_ds_full._labels), torch.asarray(support_classes)))[0]\n",
        "\n",
        "    train_ds_subset_support = torch.utils.data.Subset(train_ds_full, train_indxs_support)\n",
        "    val_ds_subset_support = torch.utils.data.Subset(val_ds_full, val_indxs_support)\n",
        "    test_ds_subset_support = torch.utils.data.Subset(test_ds_full, test_indxs_support)\n",
        "\n",
        "    merged_dataset = ConcatDataset([train_ds_subset_support, val_ds_subset_support, test_ds_subset_support])\n",
        "    ### A, B\n",
        "    train_ds_support, test_ds_support = torch.utils.data.random_split(merged_dataset, [0.75, 0.25], generator=torch.Generator().manual_seed(42))\n",
        "    ###\n",
        "\n",
        "    train_indxs_query = torch.where(torch.isin(torch.tensor(train_ds_full._labels), torch.asarray(query_classes)))[0]\n",
        "    N = 10\n",
        "    starting_indices = np.arange(0, len(train_indxs_query), N)\n",
        "    train_indxs_query = np.hstack([train_indxs_query[i:i+K] for i in starting_indices if i + K <= len(train_indxs_query)])\n",
        "    ### C\n",
        "    train_ds_query = torch.utils.data.Subset(train_ds_full, train_indxs_query)\n",
        "    ###\n",
        "\n",
        "    val_indxs_query = torch.where(torch.isin(torch.tensor(val_ds_full._labels), torch.asarray(query_classes)))[0]\n",
        "    test_indxs_query = torch.where(torch.isin(torch.tensor(test_ds_full._labels), torch.asarray(query_classes)))[0]\n",
        "    val_ds_subset_query = torch.utils.data.Subset(val_ds_full, val_indxs_query)\n",
        "    test_ds_subset_query = torch.utils.data.Subset(test_ds_full, test_indxs_query)\n",
        "\n",
        "    test_ds_query_full = ConcatDataset([val_ds_subset_query, test_ds_subset_query])\n",
        "    ### D\n",
        "    _, test_ds_query = torch.utils.data.random_split(test_ds_query_full, [0.7, 0.3], generator=torch.Generator().manual_seed(42))\n",
        "    ###\n",
        "\n",
        "    ### E\n",
        "    test_all = ConcatDataset([test_ds_query, test_ds_support])\n",
        "\n",
        "\n",
        "    A_train_dl = DataLoader(\n",
        "        train_ds_support,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    A_test_dl = DataLoader(\n",
        "        test_ds_support,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    B_train_dl = DataLoader(\n",
        "        train_ds_query,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    B_test_dl = DataLoader(\n",
        "        test_ds_query,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    test_all = DataLoader(\n",
        "        test_all,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return A_train_dl, A_test_dl, B_train_dl, B_test_dl, test_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VoGLQNV0AOX"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqwdU4e3TDd4",
        "outputId": "6f50448c-76a4-4c24-ca95-62ed51e8b44c"
      },
      "outputs": [],
      "source": [
        "A_train_dl, A_test_dl, B_train_dl, B_test_dl, test_all = get_oxford_splits(\n",
        "    batch_size=250,\n",
        "    data_loader_seed=111,\n",
        "    pin_memory=False,\n",
        "    num_workers=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryLMC8ZYnkou"
      },
      "source": [
        "## plot output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygHE7HptnjQA"
      },
      "outputs": [],
      "source": [
        "def make_dir(dir_name: str):\n",
        "    \"\"\"\n",
        "    creates directory \"dir_name\" if it doesn't exists\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)\n",
        "\n",
        "def custom_plot_training_stats(\n",
        "        acc_hist,\n",
        "        loss_hist,\n",
        "        phase_list,\n",
        "        title: str,\n",
        "        dir: str,\n",
        "        name: str = 'acc_loss'):\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=[14, 6], dpi=100)\n",
        "\n",
        "    for phase in phase_list:\n",
        "        lowest_loss_x = np.argmin(np.array(loss_hist[phase]))\n",
        "        lowest_loss_y = loss_hist[phase][lowest_loss_x]\n",
        "\n",
        "        ax1.annotate(\"{:.4f}\".format(lowest_loss_y), [lowest_loss_x, lowest_loss_y])\n",
        "        ax1.plot(loss_hist[phase], '-x', label=f'{phase} loss', markevery = [lowest_loss_x])\n",
        "\n",
        "        ax1.set_xlabel(xlabel='epochs')\n",
        "        ax1.set_ylabel(ylabel='loss')\n",
        "\n",
        "        ax1.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
        "        ax1.legend()\n",
        "        ax1.label_outer()\n",
        "\n",
        "    # acc:\n",
        "    for phase in phase_list:\n",
        "        highest_acc_x = np.argmax(np.array(acc_hist[phase]))\n",
        "        highest_acc_y = acc_hist[phase][highest_acc_x]\n",
        "\n",
        "        ax2.annotate(\"{:.4f}\".format(highest_acc_y), [highest_acc_x, highest_acc_y])\n",
        "        ax2.plot(acc_hist[phase], '-x', label=f'{phase} acc', markevery = [highest_acc_x])\n",
        "\n",
        "        ax2.set_xlabel(xlabel='epochs')\n",
        "        ax2.set_ylabel(ylabel='acc')\n",
        "\n",
        "        ax2.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
        "        ax2.legend()\n",
        "        #ax2.label_outer()\n",
        "\n",
        "    fig.suptitle(f'{title}')\n",
        "\n",
        "    make_dir(dir)\n",
        "    plt.savefig(f'{dir}/{name}.jpg')\n",
        "    plt.clf()\n",
        "\n",
        "def plot_conf(labels, preds, title, dir_, name):\n",
        "    \"\"\"\n",
        "    labels: an [N, ] array containing true labels for N samples\n",
        "    preds: an [N, ] array containing predications for N samples\n",
        "\n",
        "    saves confusion matrix plot of the given prediction and true labels in 'dir_/name.jpg'\n",
        "    \"\"\"\n",
        "\n",
        "    conf = confusion_matrix(labels, preds)\n",
        "\n",
        "    plt.clf()\n",
        "    cm = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
        "    cmap = sns.light_palette(\"navy\", as_cmap=True)\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    sns.heatmap(cm, annot=False, cmap=cmap, fmt=\".2f\", cbar=False)\n",
        "    plt.title(f'{title}')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    make_dir(dir_)\n",
        "    plt.savefig(f'{dir_}/{name}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvgSb4Y8TDd5"
      },
      "source": [
        "## Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf-5RW_2TDd5",
        "outputId": "73d2ecc4-e2af-4140-f784-a3c9cfab568f"
      },
      "outputs": [],
      "source": [
        "len(A_train_dl.dataset), len(A_test_dl.dataset), len(B_train_dl.dataset), len(B_test_dl.dataset),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KIrJjCgTDd6",
        "outputId": "1dd164d0-05e9-454f-9071-42d2d244daf1"
      },
      "outputs": [],
      "source": [
        "len(A_train_dl), len(B_train_dl), len(B_train_dl), len(B_test_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8htOAGm_sKJ",
        "outputId": "8ef13c3c-6fe1-4847-af86-3752e99bf16d"
      },
      "outputs": [],
      "source": [
        "A_train_dl.dataset[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfSjAZ5ITDd6",
        "outputId": "850bb547-c163-4408-cd12-d3405276c6bd"
      },
      "outputs": [],
      "source": [
        "for data_indx, (input, target) in enumerate(A_train_dl.dataset):\n",
        "    if data_indx < 3:\n",
        "        print('data index: ', data_indx)\n",
        "        print('input: ', input.shape, ', type of input: ', type(input))\n",
        "        print('target: ', target, ', type of target: ', type(target))\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X58_xpiTDd8"
      },
      "source": [
        "## Making network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mOnovmoTDd8"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=80):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # conv1\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # conv2\n",
        "        self.conv2_1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2_1 = nn.BatchNorm2d(64)\n",
        "        self.relu2_1 = nn.ReLU()\n",
        "\n",
        "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2_2 = nn.BatchNorm2d(64)\n",
        "        self.relu2_2 = nn.ReLU()\n",
        "\n",
        "        self.conv2_3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2_3 = nn.BatchNorm2d(64)\n",
        "        self.relu2_3 = nn.ReLU()\n",
        "\n",
        "        self.conv2_4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2_4 = nn.BatchNorm2d(64)\n",
        "        self.relu2_4 = nn.ReLU()\n",
        "\n",
        "        # pool1\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # conv3\n",
        "        self.conv3_1 = nn.Conv2d(64, 96, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3_1 = nn.BatchNorm2d(96)\n",
        "        self.relu3_1 = nn.ReLU()\n",
        "\n",
        "        self.conv3_2 = nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3_2 = nn.BatchNorm2d(96)\n",
        "        self.relu3_2 = nn.ReLU()\n",
        "\n",
        "        self.conv3_3 = nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3_3 = nn.BatchNorm2d(96)\n",
        "        self.relu3_3 = nn.ReLU()\n",
        "\n",
        "        self.conv3_4 = nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3_4 = nn.BatchNorm2d(96)\n",
        "        self.relu3_4 = nn.ReLU()\n",
        "\n",
        "        # pool2\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # conv4\n",
        "        self.conv4_1 = nn.Conv2d(96, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4_1 = nn.BatchNorm2d(128)\n",
        "        self.relu4_1 = nn.ReLU()\n",
        "\n",
        "        self.conv4_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4_2 = nn.BatchNorm2d(128)\n",
        "        self.relu4_2 = nn.ReLU()\n",
        "\n",
        "        self.conv4_3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4_3 = nn.BatchNorm2d(128)\n",
        "        self.relu4_3 = nn.ReLU()\n",
        "\n",
        "        self.conv4_4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4_4 = nn.BatchNorm2d(128)\n",
        "        self.relu4_4 = nn.ReLU()\n",
        "\n",
        "        # pool3\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # conv5\n",
        "        self.conv5_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5_1 = nn.BatchNorm2d(256)\n",
        "        self.relu5_1 = nn.ReLU()\n",
        "\n",
        "        self.conv5_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5_2 = nn.BatchNorm2d(256)\n",
        "        self.relu5_2 = nn.ReLU()\n",
        "\n",
        "        self.conv5_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5_3 = nn.BatchNorm2d(256)\n",
        "        self.relu5_3 = nn.ReLU()\n",
        "\n",
        "        self.conv5_4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5_4 = nn.BatchNorm2d(256)\n",
        "        self.relu5_4 = nn.ReLU()\n",
        "\n",
        "        # pool4\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # foolly connected\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(256*4*4, num_classes)\n",
        "\n",
        "    def forward(self, inputs, debug=False):\n",
        "        # conv 1\n",
        "        conv1 = self.conv1(inputs)\n",
        "        bn1 = self.bn1(conv1)\n",
        "        relu1 = self.relu1(bn1)\n",
        "\n",
        "        # conv 2\n",
        "        conv2_1 = self.conv2_1(relu1)\n",
        "        bn2_1 = self.bn2_1(conv2_1)\n",
        "        relu2_1 = self.relu2_1(bn2_1)\n",
        "\n",
        "        conv2_2 = self.conv2_2(relu2_1)\n",
        "        bn2_2 = self.bn2_2(conv2_2)\n",
        "        relu2_2 = self.relu2_2(bn2_2)\n",
        "\n",
        "        conv2_3 = self.conv2_3(relu2_2)\n",
        "        bn2_3 = self.bn2_3(conv2_3)\n",
        "        relu2_3 = self.relu2_3(bn2_3)\n",
        "\n",
        "        conv2_4 = self.conv2_4(relu2_3)\n",
        "        bn2_4 = self.bn2_4(conv2_4)\n",
        "        relu2_4 = self.relu2_4(bn2_4)\n",
        "\n",
        "        # pool 1\n",
        "        pool1 = self.pool1(relu2_4)\n",
        "\n",
        "        # conv 3\n",
        "        conv3_1 = self.conv3_1(pool1)\n",
        "        bn3_1 = self.bn3_1(conv3_1)\n",
        "        relu3_1 = self.relu3_1(bn3_1)\n",
        "\n",
        "        conv3_2 = self.conv3_2(relu3_1)\n",
        "        bn3_2 = self.bn3_2(conv3_2)\n",
        "        relu3_2 = self.relu3_2(bn3_2)\n",
        "\n",
        "        conv3_3 = self.conv3_3(relu3_2)\n",
        "        bn3_3 = self.bn3_3(conv3_3)\n",
        "        relu3_3 = self.relu3_3(bn3_3)\n",
        "\n",
        "        conv3_4 = self.conv3_4(relu3_3)\n",
        "        bn3_4 = self.bn3_4(conv3_4)\n",
        "        relu3_4 = self.relu3_4(bn3_4)\n",
        "\n",
        "        # pool 2\n",
        "        pool2 = self.pool2(relu3_4)\n",
        "\n",
        "        # conv 4\n",
        "        conv4_1 = self.conv4_1(pool2)\n",
        "        bn4_1 = self.bn4_1(conv4_1)\n",
        "        relu4_1 = self.relu4_1(bn4_1)\n",
        "\n",
        "        conv4_2 = self.conv4_2(relu4_1)\n",
        "        bn4_2 = self.bn4_2(conv4_2)\n",
        "        relu4_2 = self.relu4_2(bn4_2)\n",
        "\n",
        "        conv4_3 = self.conv4_3(relu4_2)\n",
        "        bn4_3 = self.bn4_3(conv4_3)\n",
        "        relu4_3 = self.relu4_3(bn4_3)\n",
        "\n",
        "        conv4_4 = self.conv4_4(relu4_3)\n",
        "        bn4_4 = self.bn4_4(conv4_4)\n",
        "        relu4_4 = self.relu4_4(bn4_4)\n",
        "\n",
        "        # pool 3\n",
        "        pool3 = self.pool3(relu4_4)\n",
        "\n",
        "        # conv 5\n",
        "        conv5_1 = self.conv5_1(pool3)\n",
        "        bn5_1 = self.bn5_1(conv5_1)\n",
        "        relu5_1 = self.relu5_1(bn5_1)\n",
        "\n",
        "        conv5_2 = self.conv5_2(relu5_1)\n",
        "        bn5_2 = self.bn5_2(conv5_2)\n",
        "        relu5_2 = self.relu5_2(bn5_2)\n",
        "\n",
        "        conv5_3 = self.conv5_3(relu5_2)\n",
        "        bn5_3 = self.bn5_3(conv5_3)\n",
        "        relu5_3 = self.relu5_3(bn5_3)\n",
        "\n",
        "        conv5_4 = self.conv5_4(relu5_3)\n",
        "        bn5_4 = self.bn5_4(conv5_4)\n",
        "        relu5_4 = self.relu5_4(bn5_4)\n",
        "\n",
        "        # pool 4\n",
        "        pool4 = self.pool4(relu5_4)\n",
        "\n",
        "        # fc\n",
        "        flatten = self.flatten(pool4)\n",
        "        fc = self.fc(flatten)\n",
        "\n",
        "        return(fc)\n",
        "\n",
        "    def freeze_except_FC_layer(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.fc.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def freeze_FC_except_last_20_neurons(self):\n",
        "        # Freeze all parameters in the fully connected layer\n",
        "        for param in self.fc.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the parameters of the last 20 neurons\n",
        "        self.fc.weight.requires_grad = True\n",
        "        self.fc.bias.requires_grad = True\n",
        "        self.fc.weight.data[:80, :].requires_grad = False\n",
        "        self.fc.bias.data[:80].requires_grad = False\n",
        "\n",
        "    def unfreeze_all_layers(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_GDS8aMyxlJ"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgXu7x7bzIJr"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model: nn.Module, optim: torch.optim.Optimizer,\n",
        "                    dataloader: DataLoader, loss_fn):\n",
        "\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_indx, (inputs, targets) in enumerate(dataloader): # Get a batch of Data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(inputs) # Forward Pass\n",
        "        loss = loss_fn(outputs, targets) # Compute Loss\n",
        "\n",
        "        loss.backward() # Compute Gradients\n",
        "        optim.step() # Update parameters\n",
        "        optim.zero_grad() # zero the parameter's gradients\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        running_corrects += torch.sum(preds == targets).cpu()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_acc = (running_corrects / num_samples) * 100\n",
        "    print(\"train: \", running_corrects, num_samples)\n",
        "    epoch_loss = (running_loss / num_batches)\n",
        "\n",
        "    return epoch_acc, epoch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9F2sjhX2e9t"
      },
      "source": [
        "## Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPiBHmCp0jyC"
      },
      "outputs": [],
      "source": [
        "def test_model(model: nn.Module,\n",
        "               dataloader: DataLoader, loss_fn):\n",
        "\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    true_labels = torch.empty(0).to(device)\n",
        "    pred_labels = torch.empty(0).to(device)\n",
        "\n",
        "    # we call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_indx, (inputs, targets) in enumerate(dataloader): # Get a batch of Data\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs) # Forward Pass\n",
        "            loss = loss_fn(outputs, targets) # Compute Loss\n",
        "\n",
        "            _, preds = torch.max(outputs, 1) #\n",
        "            running_corrects += torch.sum(preds == targets).cpu()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            true_labels = torch.cat([true_labels, targets], dim=0)\n",
        "            pred_labels = torch.cat([pred_labels, preds], dim=0)\n",
        "\n",
        "    test_acc = (running_corrects / num_samples) * 100\n",
        "    print(\"test: \", running_corrects, num_samples)\n",
        "    test_loss = (running_loss / num_batches)\n",
        "\n",
        "    return test_acc, test_loss, true_labels, pred_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZzZXJI4ntJj"
      },
      "source": [
        "## confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB8RfTh7nryL"
      },
      "outputs": [],
      "source": [
        "def plot_conf(labels, preds, title, dir_, name):\n",
        "    \"\"\"\n",
        "    labels: an [N, ] array containing true labels for N samples\n",
        "    preds: an [N, ] array containing predications for N samples\n",
        "\n",
        "    saves confusion matrix plot of the given prediction and true labels in 'dir_/name.jpg'\n",
        "    \"\"\"\n",
        "\n",
        "    conf = confusion_matrix(labels, preds)\n",
        "\n",
        "    plt.clf()\n",
        "    cm = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
        "    cmap = sns.light_palette(\"navy\", as_cmap=True)\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    sns.heatmap(cm, annot=False, cmap=cmap, fmt=\".2f\", cbar=False)\n",
        "    plt.title(f'{title}')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    make_dir(dir_)\n",
        "    plt.savefig(f'{dir_}/{name}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjWz5rRn2jlp"
      },
      "source": [
        "## Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAexh8NL1SOm"
      },
      "outputs": [],
      "source": [
        "def evaluate_A():\n",
        "    num_epochs = 30\n",
        "    learning_rate = 0.0005\n",
        "\n",
        "    full_dataloaders = {\n",
        "        'train': A_train_dl,\n",
        "        'test': A_test_dl\n",
        "    }\n",
        "\n",
        "    model = CNN(80)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    acc_history = {'train': [], 'test': []}\n",
        "    loss_history = {'train': [], 'test': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_acc, train_loss = train_one_epoch(model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
        "        test_acc, test_loss, true_labels, pred_labels = test_model(model=model, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n",
        "\n",
        "        acc_history['train'].append(train_acc)\n",
        "        acc_history['test'].append(test_acc)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        loss_history['test'].append(test_loss)\n",
        "\n",
        "        plot_conf(true_labels.cpu(), pred_labels.cpu(), f\"epoch num :{epoch}\", './ph1_matrix', f'epoch_{epoch}')\n",
        "\n",
        "        print(f\"---------< epoch: {epoch} >---------\")\n",
        "\n",
        "    # save model\n",
        "    model_path = './CNN_model_ph1.pth'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # plot accuracy and loss\n",
        "    custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots')\n",
        "\n",
        "    # show the details of model\n",
        "    batch_size=250\n",
        "    print(summary(model, input_size=(batch_size, 3, 64, 64)))\n",
        "\n",
        "    return (acc_history, loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3tgjkkym8zzJ",
        "outputId": "89b4803e-6185-4cec-fbf9-e7d22ec6902a"
      },
      "outputs": [],
      "source": [
        "acc_history, loss_history = evaluate_A()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwMi4-EZGXOv"
      },
      "source": [
        "# phase 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JhLLyyXzIsu"
      },
      "source": [
        "## part1 - evaluate on B dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMUYXvflltgq"
      },
      "outputs": [],
      "source": [
        "def evaluate_B_1():\n",
        "    num_epochs = 60\n",
        "    learning_rate = 0.0003\n",
        "\n",
        "    full_dataloaders = {\n",
        "        'train': B_train_dl,\n",
        "        'test': test_all\n",
        "    }\n",
        "\n",
        "    # loading phase1 model\n",
        "    model1 = CNN(80)\n",
        "    model2 = CNN(100)\n",
        "    model1 = model1.to(device)\n",
        "    model2 = model2.to(device)\n",
        "    model_path = './CNN_model_ph1.pth'\n",
        "    model1.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # Copy parameters from model1 to model2 (excluding the fully connected layer)\n",
        "    for layer1, layer2 in zip(model1.children(), model2.children()):\n",
        "        if isinstance(layer1, nn.Linear):\n",
        "            continue  # Skip copying parameters for fully connected layer\n",
        "\n",
        "        # Copy parameters from layer1 to layer2\n",
        "        for param1, param2 in zip(layer1.parameters(), layer2.parameters()):\n",
        "            param2.data.copy_(param1.data)\n",
        "\n",
        "    model2.fc.weight.data[:80, :] = model1.fc.weight.data\n",
        "    model2.fc.bias.data[:80] = model1.fc.bias.data\n",
        "    # model2.freeze_layers()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model2.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    acc_history = {'train': [], 'test': []}\n",
        "    loss_history = {'train': [], 'test': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_acc, train_loss = train_one_epoch(model=model2, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
        "        test_acc, test_loss, true_labels, pred_labels = test_model(model=model2, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n",
        "\n",
        "        acc_history['train'].append(train_acc)\n",
        "        acc_history['test'].append(test_acc)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        loss_history['test'].append(test_loss)\n",
        "\n",
        "        plot_conf(true_labels.cpu(), pred_labels.cpu(), f\"epoch num :{epoch}\", './ph2_1_matrix', f'epoch_{epoch}')\n",
        "\n",
        "        print(f\"---------< epoch: {epoch} >---------\")\n",
        "\n",
        "    # save model\n",
        "    model_path = './CNN_model_ph2_1.pth'\n",
        "    torch.save(model2.state_dict(), model_path)\n",
        "\n",
        "    # plot accuracy and loss\n",
        "    custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots_2_1')\n",
        "\n",
        "    # show the details of model\n",
        "    batch_size=250\n",
        "    print(summary(model2, input_size=(batch_size, 3, 64, 64)))\n",
        "\n",
        "    return (acc_history, loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aFW6STsOzDFc",
        "outputId": "c6ed1bc7-17ec-4fd3-d950-24035109b80c"
      },
      "outputs": [],
      "source": [
        "acc_history2_1, loss_history2_1 = evaluate_B_1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-x2t4K1AJp3"
      },
      "source": [
        "## check network copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BLD7hWYzz6r",
        "outputId": "38c89d26-2582-482e-da8a-d6e53ea4b666"
      },
      "outputs": [],
      "source": [
        "# loading phase1 model\n",
        "model1 = CNN(80)\n",
        "model2 = CNN(100)\n",
        "model1 = model1.to(device)\n",
        "model2 = model2.to(device)\n",
        "model_path = './CNN_model_ph1.pth'\n",
        "model1.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Copy parameters from model1 to model2 (excluding the fully connected layer)\n",
        "for layer1, layer2 in zip(model1.children(), model2.children()):\n",
        "    if isinstance(layer1, nn.Linear):\n",
        "        continue  # Skip copying parameters for fully connected layer\n",
        "\n",
        "    # Copy parameters from layer1 to layer2\n",
        "    for param1, param2 in zip(layer1.parameters(), layer2.parameters()):\n",
        "        param2.data.copy_(param1.data)\n",
        "\n",
        "model2.fc.weight.data[:80, :] = model1.fc.weight.data\n",
        "model2.fc.bias.data[:80] = model1.fc.bias.data\n",
        "# model2.freeze_layers()\n",
        "\n",
        "# -----------------\n",
        "# Compare all layers except the fully connected layer\n",
        "def are_layers_except_fc_equal(model1, model2):\n",
        "    for layer1, layer2 in zip(model1.children(), model2.children()):\n",
        "        if isinstance(layer1, nn.Linear):\n",
        "            continue  # Skip the fully connected layer\n",
        "        if not all(torch.equal(param1, param2) for param1, param2 in zip(layer1.parameters(), layer2.parameters())):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Check if all layers except the fully connected layer are equal\n",
        "layers_except_fc_equal = are_layers_except_fc_equal(model1, model2)\n",
        "print(f\"Are all layers except the fully connected layer equal? {layers_except_fc_equal}\")\n",
        "\n",
        "# -----------------\n",
        "# Count the number of equal parameters\n",
        "equal_parameters_count = 0\n",
        "total_parameters_count = 0\n",
        "\n",
        "for param1, param2 in zip(model1.fc.weight.data, model2.fc.weight.data):\n",
        "    total_parameters_count += param1.numel()\n",
        "\n",
        "    # Check if parameters are equal\n",
        "    if torch.equal(param1, param2):\n",
        "        equal_parameters_count += param1.numel()\n",
        "\n",
        "# Calculate the percentage of equal parameters\n",
        "percentage_equal_parameters = (equal_parameters_count / total_parameters_count) * 100\n",
        "\n",
        "print(f\"Number of equal parameters in the fully connected layer: {equal_parameters_count}\")\n",
        "print(f\"Total number of parameters in the fully connected layer: {total_parameters_count}\")\n",
        "print(f\"Percentage of equal parameters: {percentage_equal_parameters:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5oc7j3UoeMi"
      },
      "source": [
        "## part2 - evaluate on B dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GE3s3bfokKQ"
      },
      "outputs": [],
      "source": [
        "def evaluate_B_2():\n",
        "    num_epochs = 60\n",
        "    learning_rate = 0.0003\n",
        "\n",
        "    full_dataloaders = {\n",
        "        'train': B_train_dl,\n",
        "        'test': test_all\n",
        "    }\n",
        "\n",
        "    # loading phase1 model\n",
        "    model1 = CNN(80)\n",
        "    model2 = CNN(100)\n",
        "    model1 = model1.to(device)\n",
        "    model2 = model2.to(device)\n",
        "    model_path = './CNN_model_ph1.pth'\n",
        "    model1.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # Copy parameters from model1 to model2 (excluding the fully connected layer)\n",
        "    for layer1, layer2 in zip(model1.children(), model2.children()):\n",
        "        if isinstance(layer1, nn.Linear):\n",
        "            continue  # Skip copying parameters for fully connected layer\n",
        "\n",
        "        # Copy parameters from layer1 to layer2\n",
        "        for param1, param2 in zip(layer1.parameters(), layer2.parameters()):\n",
        "            param2.data.copy_(param1.data)\n",
        "\n",
        "    model2.fc.weight.data[:80, :] = model1.fc.weight.data\n",
        "    model2.fc.bias.data[:80] = model1.fc.bias.data\n",
        "\n",
        "    # freeze all the layers exept fully connected layer\n",
        "    model2.freeze_except_FC_layer()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model2.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    acc_history = {'train': [], 'test': []}\n",
        "    loss_history = {'train': [], 'test': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_acc, train_loss = train_one_epoch(model=model2, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
        "        test_acc, test_loss, true_labels, pred_labels = test_model(model=model2, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n",
        "\n",
        "        acc_history['train'].append(train_acc)\n",
        "        acc_history['test'].append(test_acc)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        loss_history['test'].append(test_loss)\n",
        "\n",
        "        plot_conf(true_labels.cpu(), pred_labels.cpu(), f\"epoch num :{epoch}\", './ph2_2_matrix', f'epoch_{epoch}')\n",
        "\n",
        "        print(f\"---------< epoch: {epoch} >---------\")\n",
        "\n",
        "    # # save model\n",
        "    model_path = './CNN_model_ph2_2.pth'\n",
        "    torch.save(model2.state_dict(), model_path)\n",
        "\n",
        "    # plot accuracy and loss\n",
        "    custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots_2_2')\n",
        "\n",
        "    # show the details of model\n",
        "    batch_size=250\n",
        "    print(summary(model2, input_size=(batch_size, 3, 64, 64)))\n",
        "\n",
        "    return (acc_history, loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6lpayPLMpFjT",
        "outputId": "938363b2-bd82-4895-e213-00eb4a21d464"
      },
      "outputs": [],
      "source": [
        "acc_history2_2, loss_history2_2 = evaluate_B_2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6ETnyKdVskg"
      },
      "source": [
        "## part3 - evaluate on B dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK_Yxq1qVsMG"
      },
      "outputs": [],
      "source": [
        "def evaluate_B_3():\n",
        "    num_epochs = 60\n",
        "    learning_rate = 0.0003\n",
        "\n",
        "    full_dataloaders = {\n",
        "        'train': B_train_dl,\n",
        "        'test': test_all\n",
        "    }\n",
        "\n",
        "    # loading phase1 model\n",
        "    model1 = CNN(80)\n",
        "    model2 = CNN(100)\n",
        "    model1 = model1.to(device)\n",
        "    model2 = model2.to(device)\n",
        "    model_path = './CNN_model_ph1.pth'\n",
        "    model1.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # Copy parameters from model1 to model2 (excluding the fully connected layer)\n",
        "    for layer1, layer2 in zip(model1.children(), model2.children()):\n",
        "        if isinstance(layer1, nn.Linear):\n",
        "            continue  # Skip copying parameters for fully connected layer\n",
        "\n",
        "        # Copy parameters from layer1 to layer2\n",
        "        for param1, param2 in zip(layer1.parameters(), layer2.parameters()):\n",
        "            param2.data.copy_(param1.data)\n",
        "\n",
        "    model2.fc.weight.data[:80, :] = model1.fc.weight.data\n",
        "    model2.fc.bias.data[:80] = model1.fc.bias.data\n",
        "\n",
        "    # freeze all the layers exept last 20 neurons of fully connected layer\n",
        "    model2.freeze_except_FC_layer()\n",
        "    model2.freeze_FC_except_last_20_neurons()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model2.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    acc_history = {'train': [], 'test': []}\n",
        "    loss_history = {'train': [], 'test': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_acc, train_loss = train_one_epoch(model=model2, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
        "        test_acc, test_loss, true_labels, pred_labels = test_model(model=model2, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n",
        "\n",
        "        acc_history['train'].append(train_acc)\n",
        "        acc_history['test'].append(test_acc)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        loss_history['test'].append(test_loss)\n",
        "\n",
        "        plot_conf(true_labels.cpu(), pred_labels.cpu(), f\"epoch num :{epoch}\", './ph2_3_matrix', f'epoch_{epoch}')\n",
        "\n",
        "        print(f\"---------< epoch: {epoch} >---------\")\n",
        "\n",
        "    # # save model\n",
        "    model_path = './CNN_model_ph2_3.pth'\n",
        "    torch.save(model2.state_dict(), model_path)\n",
        "\n",
        "    # plot accuracy and loss\n",
        "    custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots_2_3')\n",
        "\n",
        "    # show the details of model\n",
        "    batch_size=250\n",
        "    print(summary(model2, input_size=(batch_size, 3, 64, 64)))\n",
        "\n",
        "    return (acc_history, loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jRgJ2IUSVu-w",
        "outputId": "1e902374-9b28-4fda-db39-a0b72a3ad2b2"
      },
      "outputs": [],
      "source": [
        "acc_history2_2, loss_history2_2 = evaluate_B_3()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhwZ1kDmbHhH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6VoGLQNV0AOX",
        "cvgSb4Y8TDd5",
        "8X58_xpiTDd8",
        "C9F2sjhX2e9t"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
